{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stacked_CNN_gloVe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HOtZ5l_U4FyWfPZpYmAM3qWrDPdKMSya",
      "authorship_tag": "ABX9TyMGZo6aTcvSe0RTMUZ0oHCx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "trD-9g79UzrQ"
      },
      "source": [
        "# Import statements\n",
        "import pandas as pd\n",
        "from nltk import word_tokenize\n",
        "from numpy import array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKqPoZiXboFq",
        "outputId": "634e3627-d6ba-433d-941e-2aa0438911e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Loading in preproccessed data\n",
        "PATH = \"/content/drive/My Drive/Data/\"\n",
        "\n",
        "# create dataframes and keep only necessary features to join dataframes\n",
        "data = pd.read_csv(PATH + \"final_preproccessed_data.csv\", lineterminator='\\n')\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Muslims BUSTED: They Stole Millions In Gov’t B...</td>\n",
              "      <td>Print They should pay all the back all the mon...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Re: Why Did Attorney General Loretta Lynch Ple...</td>\n",
              "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BREAKING: Weiner Cooperating With FBI On Hilla...</td>\n",
              "      <td>Red State : \\nFox News Sunday reported this mo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
              "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...</td>\n",
              "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29484</th>\n",
              "      <td>Travel deals: Get $1200 of air credit for two ...</td>\n",
              "      <td>APT is offering savings on its new Cape York a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29485</th>\n",
              "      <td>Hospital patients 'more likely to die if admit...</td>\n",
              "      <td>Patients admitted to NHS hospitals on weekends...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29486</th>\n",
              "      <td>A Taiwanese Recycler's Belief That All Waste I...</td>\n",
              "      <td>TAIPEI, Taiwan , Sept. 8, 2015 /PRNewswire/ --...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29487</th>\n",
              "      <td>Season curtain raiser is ideal way to honour John</td>\n",
              "      <td>Blackburn Sunday League John Haydock Memorial ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29488</th>\n",
              "      <td>Four Cooper Standard Facilities Promote North ...</td>\n",
              "      <td>NOVI, Mich. , Sept. 30, 2015 /PRNewswire/ -- F...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29489 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... label\n",
              "0      Muslims BUSTED: They Stole Millions In Gov’t B...  ...     1\n",
              "1      Re: Why Did Attorney General Loretta Lynch Ple...  ...     1\n",
              "2      BREAKING: Weiner Cooperating With FBI On Hilla...  ...     1\n",
              "3      PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...  ...     1\n",
              "4      FANTASTIC! TRUMP'S 7 POINT PLAN To Reform Heal...  ...     1\n",
              "...                                                  ...  ...   ...\n",
              "29484  Travel deals: Get $1200 of air credit for two ...  ...     0\n",
              "29485  Hospital patients 'more likely to die if admit...  ...     0\n",
              "29486  A Taiwanese Recycler's Belief That All Waste I...  ...     0\n",
              "29487  Season curtain raiser is ideal way to honour John  ...     0\n",
              "29488  Four Cooper Standard Facilities Promote North ...  ...     0\n",
              "\n",
              "[29489 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36438kdFcw4S"
      },
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_jJPPNU-kxQ"
      },
      "source": [
        "\n",
        "# data['text_tokens'] = [word_tokenize(text) for text in data.text]\n",
        "# # data['title_tokens'] = [word_tokenize(title) for title in data.title]\n",
        "\n",
        "# data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1UWR36Ybk_G"
      },
      "source": [
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "tokenizer = create_tokenizer(data_train['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVeEaN2ibT94",
        "outputId": "49f15a77-2db8-4474-e390-a18284fa6ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "maxlen = max_length(data_train['text'])\n",
        "\n",
        "print('Max document length: %d' % maxlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 3194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09d8R47WYrZH",
        "outputId": "f2fe3f78-ca9a-44ec-bbe8-97f3423ff1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('Vocabulary size: %d' % vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 221431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz5e2Xm9dxwO",
        "outputId": "4e9abe2b-ce40-4029-b4c9-492ca71cc964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  # pad encoded sequences\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded\n",
        "\n",
        "encoded_data = encode_text(tokenizer, data_train['text'], maxlen)\n",
        "print(encoded_data.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26540, 3194)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2lvplswWyuK",
        "outputId": "892ff0ee-e91c-4c83-8ef2-045c40d2e206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "EMBEDDING_FILE = '/content/drive/My Drive/Data/glove.6B.100d.txt'\n",
        "\n",
        "def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "# embeddings_matrix = np.zeros((max_features+1, 100));\n",
        "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, embed_size))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o42bxuT7fUrB"
      },
      "source": [
        "def define_model(length, vocab_size):\n",
        "\t# channel 1\n",
        "\tinputs1 = Input(shape=(length,))\n",
        "\tembedding1 = Embedding(vocab_size, 100, weights=[embedding_matrix],trainable=False)(inputs1)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tinputs2 = Input(shape=(length,))\n",
        "\tembedding2 = Embedding(vocab_size, 100, weights=[embedding_matrix],trainable=False)(inputs2)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tinputs3 = Input(shape=(length,))\n",
        "\tembedding3 = Embedding(vocab_size, 100, weights=[embedding_matrix],trainable=False)(inputs3)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(10, activation='relu')(merged)\n",
        "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "\treturn model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI6gq2kt9iCD"
      },
      "source": [
        "encoded_data_testing = encode_text(tokenizer, data_test['text'], maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzTRY0rifcRa",
        "outputId": "ae451300-fe02-4291-9769-ee00be855e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define model\n",
        "model = define_model(maxlen, vocab_size)\n",
        "# fit model\n",
        "model.fit([encoded_data, encoded_data, encoded_data], array(data_train['label']), epochs=6, batch_size=16)\n",
        "# save the model\n",
        "model.save('model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 3194)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 3194)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 3194)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 3194, 100)    22143100    input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 3194, 100)    22143100    input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 3194, 100)    22143100    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 3191, 32)     12832       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 3189, 32)     19232       embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 3187, 32)     25632       embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 3191, 32)     0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 3189, 32)     0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 3187, 32)     0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 1595, 32)     0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 1594, 32)     0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 1593, 32)     0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 51040)        0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 51008)        0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 50976)        0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 153024)       0           flatten_3[0][0]                  \n",
            "                                                                 flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           1530250     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            11          dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 68,017,257\n",
            "Trainable params: 1,587,957\n",
            "Non-trainable params: 66,429,300\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/6\n",
            "1659/1659 [==============================] - 31s 19ms/step - loss: 0.7074 - accuracy: 0.5649\n",
            "Epoch 2/6\n",
            "1659/1659 [==============================] - 31s 19ms/step - loss: 0.6843 - accuracy: 0.5668\n",
            "Epoch 3/6\n",
            "1659/1659 [==============================] - 31s 19ms/step - loss: 0.6842 - accuracy: 0.5668\n",
            "Epoch 4/6\n",
            "1659/1659 [==============================] - 31s 19ms/step - loss: 0.6842 - accuracy: 0.5668\n",
            "Epoch 5/6\n",
            "1659/1659 [==============================] - 31s 19ms/step - loss: 0.6842 - accuracy: 0.5668\n",
            "Epoch 6/6\n",
            "1659/1659 [==============================] - 31s 19ms/step - loss: 0.6843 - accuracy: 0.5668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU3mXTwNnYo3",
        "outputId": "26285603-d2f0-4cd8-a24f-e852f8d9cca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoded_data_testing = encode_text(tokenizer, data_test['text'], maxlen)\n",
        "\n",
        "# evaluate model on training dataset\n",
        "loss_train, acc_train = model.evaluate([encoded_data, encoded_data, encoded_data], array(data_train['label']), verbose=0)\n",
        "print('Train Accuracy: %f' % (acc_train*100))\n",
        " \n",
        "# evaluate model on test dataset dataset\n",
        "loss_test, acc_test = model.evaluate([encoded_data_testing, encoded_data_testing, encoded_data_testing],array(data_test['label']), verbose=0)\n",
        "print('Test Accuracy: %f' % (acc_test*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 56.676716\n",
            "Test Accuracy: 57.612753\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}