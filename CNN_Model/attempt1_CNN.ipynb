{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attempt1_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HOtZ5l_U4FyWfPZpYmAM3qWrDPdKMSya",
      "authorship_tag": "ABX9TyMGgeyLmmmnpI2Fq+f37a8S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "trD-9g79UzrQ"
      },
      "source": [
        "# Import statements\n",
        "import pandas as pd\n",
        "from nltk import word_tokenize\n",
        "from numpy import array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKqPoZiXboFq",
        "outputId": "ee913790-83a1-4c73-8ac8-168f1aba5d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Loading in preproccessed data\n",
        "PATH = \"/content/drive/My Drive/Data/\"\n",
        "\n",
        "# create dataframes and keep only necessary features to join dataframes\n",
        "data = pd.read_csv(PATH + \"final_preproccessed_data.csv\", lineterminator='\\n')\n",
        "\n",
        "data"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>muslims busted stole millions gov ’ benefits</td>\n",
              "      <td>print pay back money plus interest entire fami...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>attorney general loretta lynch plead fifth</td>\n",
              "      <td>attorney general loretta lynch plead fifth bar...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>breaking weiner cooperating fbi hillary email ...</td>\n",
              "      <td>red state fox news sunday reported morning ant...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pin drop speech father daughter kidnapped kill...</td>\n",
              "      <td>email kayla mueller prisoner tortured isis cha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fantastic trumps 7 point plan reform healthcar...</td>\n",
              "      <td>email healthcare reform make america great sin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29483</th>\n",
              "      <td>travel deals get 1200 air credit two apt cookt...</td>\n",
              "      <td>apt offering savings new cape york outback wil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29484</th>\n",
              "      <td>atlantis</td>\n",
              "      <td>week ago today september 4th robby went atlant...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29485</th>\n",
              "      <td>taiwanese recyclers belief waste simply mispla...</td>\n",
              "      <td>taipei taiwan sept 8 2015 prnewswire recent ye...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29486</th>\n",
              "      <td>season curtain raiser ideal way honour john</td>\n",
              "      <td>blackburn sunday league john haydock memorial ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29487</th>\n",
              "      <td>four cooper standard facilities promote north ...</td>\n",
              "      <td>novi mich sept 30 2015 prnewswire four cooper ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29488 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... label\n",
              "0           muslims busted stole millions gov ’ benefits  ...     1\n",
              "1             attorney general loretta lynch plead fifth  ...     1\n",
              "2      breaking weiner cooperating fbi hillary email ...  ...     1\n",
              "3      pin drop speech father daughter kidnapped kill...  ...     1\n",
              "4      fantastic trumps 7 point plan reform healthcar...  ...     1\n",
              "...                                                  ...  ...   ...\n",
              "29483  travel deals get 1200 air credit two apt cookt...  ...     0\n",
              "29484                                           atlantis  ...     0\n",
              "29485  taiwanese recyclers belief waste simply mispla...  ...     0\n",
              "29486        season curtain raiser ideal way honour john  ...     0\n",
              "29487  four cooper standard facilities promote north ...  ...     0\n",
              "\n",
              "[29488 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36438kdFcw4S"
      },
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_jJPPNU-kxQ",
        "outputId": "5c67efa3-c015-46b1-ae64-79ffa31c33d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "\n",
        "# data['text_tokens'] = [word_tokenize(text) for text in data.text]\n",
        "# # data['title_tokens'] = [word_tokenize(title) for title in data.title]\n",
        "\n",
        "# data"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>muslims busted stole millions gov ’ benefits</td>\n",
              "      <td>print pay back money plus interest entire fami...</td>\n",
              "      <td>1</td>\n",
              "      <td>[print, pay, back, money, plus, interest, enti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>attorney general loretta lynch plead fifth</td>\n",
              "      <td>attorney general loretta lynch plead fifth bar...</td>\n",
              "      <td>1</td>\n",
              "      <td>[attorney, general, loretta, lynch, plead, fif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>breaking weiner cooperating fbi hillary email ...</td>\n",
              "      <td>red state fox news sunday reported morning ant...</td>\n",
              "      <td>1</td>\n",
              "      <td>[red, state, fox, news, sunday, reported, morn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pin drop speech father daughter kidnapped kill...</td>\n",
              "      <td>email kayla mueller prisoner tortured isis cha...</td>\n",
              "      <td>1</td>\n",
              "      <td>[email, kayla, mueller, prisoner, tortured, is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fantastic trumps 7 point plan reform healthcar...</td>\n",
              "      <td>email healthcare reform make america great sin...</td>\n",
              "      <td>1</td>\n",
              "      <td>[email, healthcare, reform, make, america, gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29483</th>\n",
              "      <td>travel deals get 1200 air credit two apt cookt...</td>\n",
              "      <td>apt offering savings new cape york outback wil...</td>\n",
              "      <td>0</td>\n",
              "      <td>[apt, offering, savings, new, cape, york, outb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29484</th>\n",
              "      <td>atlantis</td>\n",
              "      <td>week ago today september 4th robby went atlant...</td>\n",
              "      <td>0</td>\n",
              "      <td>[week, ago, today, september, 4th, robby, went...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29485</th>\n",
              "      <td>taiwanese recyclers belief waste simply mispla...</td>\n",
              "      <td>taipei taiwan sept 8 2015 prnewswire recent ye...</td>\n",
              "      <td>0</td>\n",
              "      <td>[taipei, taiwan, sept, 8, 2015, prnewswire, re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29486</th>\n",
              "      <td>season curtain raiser ideal way honour john</td>\n",
              "      <td>blackburn sunday league john haydock memorial ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[blackburn, sunday, league, john, haydock, mem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29487</th>\n",
              "      <td>four cooper standard facilities promote north ...</td>\n",
              "      <td>novi mich sept 30 2015 prnewswire four cooper ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[novi, mich, sept, 30, 2015, prnewswire, four,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>29488 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ...                                        text_tokens\n",
              "0           muslims busted stole millions gov ’ benefits  ...  [print, pay, back, money, plus, interest, enti...\n",
              "1             attorney general loretta lynch plead fifth  ...  [attorney, general, loretta, lynch, plead, fif...\n",
              "2      breaking weiner cooperating fbi hillary email ...  ...  [red, state, fox, news, sunday, reported, morn...\n",
              "3      pin drop speech father daughter kidnapped kill...  ...  [email, kayla, mueller, prisoner, tortured, is...\n",
              "4      fantastic trumps 7 point plan reform healthcar...  ...  [email, healthcare, reform, make, america, gre...\n",
              "...                                                  ...  ...                                                ...\n",
              "29483  travel deals get 1200 air credit two apt cookt...  ...  [apt, offering, savings, new, cape, york, outb...\n",
              "29484                                           atlantis  ...  [week, ago, today, september, 4th, robby, went...\n",
              "29485  taiwanese recyclers belief waste simply mispla...  ...  [taipei, taiwan, sept, 8, 2015, prnewswire, re...\n",
              "29486        season curtain raiser ideal way honour john  ...  [blackburn, sunday, league, john, haydock, mem...\n",
              "29487  four cooper standard facilities promote north ...  ...  [novi, mich, sept, 30, 2015, prnewswire, four,...\n",
              "\n",
              "[29488 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1UWR36Ybk_G"
      },
      "source": [
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "tokenizer = create_tokenizer(data_train['text'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVeEaN2ibT94",
        "outputId": "1acc288c-cb91-464f-ec20-013b23333a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "\treturn max([len(s.split()) for s in lines])\n",
        " \n",
        "max_length = max_length(data_train['text'])\n",
        "\n",
        "print('Max document length: %d' % max_length)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 2202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09d8R47WYrZH",
        "outputId": "54770a16-a756-47be-8b90-6969f7dd873e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('Vocabulary size: %d' % vocab_size)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 213373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz5e2Xm9dxwO"
      },
      "source": [
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  # pad encoded sequences\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded\n",
        "\n",
        "encoded_data = encode_text(tokenizer, data_train['text'], max_length)\n",
        "print(encoded_data.shape)\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o42bxuT7fUrB"
      },
      "source": [
        "def define_model(length, vocab_size):\n",
        "\t# channel 1\n",
        "\tinputs1 = Input(shape=(length,))\n",
        "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "\tdrop1 = Dropout(0.5)(conv1)\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "\tflat1 = Flatten()(pool1)\n",
        "\t# channel 2\n",
        "\tinputs2 = Input(shape=(length,))\n",
        "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "\tdrop2 = Dropout(0.5)(conv2)\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "\tflat2 = Flatten()(pool2)\n",
        "\t# channel 3\n",
        "\tinputs3 = Input(shape=(length,))\n",
        "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "\tdrop3 = Dropout(0.5)(conv3)\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "\tflat3 = Flatten()(pool3)\n",
        "\t# merge\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\n",
        "\t# interpretation\n",
        "\tdense1 = Dense(10, activation='relu')(merged)\n",
        "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "\t# compile\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# summarize\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "\treturn model\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzTRY0rifcRa",
        "outputId": "d3a83ad3-f5c7-4103-f507-0a4af13612f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# define model\n",
        "model = define_model(max_length, vocab_size)\n",
        "# fit model\n",
        "model.fit([encoded_data, encoded_data, encoded_data], array(data_train['label']), epochs=50, batch_size=16)\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 2202)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 2202)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 2202)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 2202, 100)    21337300    input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 2202, 100)    21337300    input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 2202, 100)    21337300    input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 2199, 32)     12832       embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 2197, 32)     19232       embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 2195, 32)     25632       embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 2199, 32)     0           conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 2197, 32)     0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 2195, 32)     0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, 1099, 32)     0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling1D) (None, 1098, 32)     0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling1D) (None, 1097, 32)     0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 35168)        0           max_pooling1d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 35136)        0           max_pooling1d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 35104)        0           max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 105408)       0           flatten_9[0][0]                  \n",
            "                                                                 flatten_10[0][0]                 \n",
            "                                                                 flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 10)           1054090     concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            11          dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 65,123,697\n",
            "Trainable params: 65,123,697\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "830/830 [==============================] - 1312s 2s/step - loss: 0.3483 - accuracy: 0.8472\n",
            "Epoch 2/50\n",
            "830/830 [==============================] - 1309s 2s/step - loss: 0.1228 - accuracy: 0.9524\n",
            "Epoch 3/50\n",
            "830/830 [==============================] - 1307s 2s/step - loss: 0.0262 - accuracy: 0.9911\n",
            "Epoch 4/50\n",
            "830/830 [==============================] - 1310s 2s/step - loss: 0.0092 - accuracy: 0.9968\n",
            "Epoch 5/50\n",
            "830/830 [==============================] - 1317s 2s/step - loss: 0.0045 - accuracy: 0.9979\n",
            "Epoch 6/50\n",
            "830/830 [==============================] - 1314s 2s/step - loss: 0.0032 - accuracy: 0.9983\n",
            "Epoch 7/50\n",
            "830/830 [==============================] - 1316s 2s/step - loss: 0.0027 - accuracy: 0.9985\n",
            "Epoch 8/50\n",
            "830/830 [==============================] - 1330s 2s/step - loss: 0.0187 - accuracy: 0.9939\n",
            "Epoch 9/50\n",
            "830/830 [==============================] - 1326s 2s/step - loss: 0.0110 - accuracy: 0.9962\n",
            "Epoch 10/50\n",
            "830/830 [==============================] - 1316s 2s/step - loss: 0.0068 - accuracy: 0.9976\n",
            "Epoch 11/50\n",
            "830/830 [==============================] - 1319s 2s/step - loss: 0.0117 - accuracy: 0.9972\n",
            "Epoch 12/50\n",
            "153/830 [====>.........................] - ETA: 18:06 - loss: 0.0018 - accuracy: 0.9994"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU3mXTwNnYo3"
      },
      "source": [
        "encoded_text_testing = encode_text(tokenizer, data_test['text'], max_length)\n",
        "\n",
        "# evaluate model on training dataset\n",
        "loss_train, acc_train = model.evaluate([encoded_data, encoded_data, encoded_data], array(data_train['label']), verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        " \n",
        "# evaluate model on test dataset dataset\n",
        "loss_test, acc_test = model.evaluate([encoded_text_testing, encoded_text_testing, encoded_text_testing],array(data_test['label']), verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}