{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LSTM",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1pCsuHteDF6tx7XvtgOG3oINH41xqxTZw",
      "authorship_tag": "ABX9TyMBEbFBJtUdE0B9Dtsm3e62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krystaldowling/MAST30034_Final_Project-/blob/preprocessing/NN_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8-xzfMlOAsU"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzA0s6lrNjvM"
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Data/final_preproccessed_data.csv', lineterminator='\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiAN2ns2Ey8X"
      },
      "source": [
        "def CV(training_text):\n",
        "\n",
        "  \"\"\"\n",
        "  Uses CountVectoriser to vectorise text.\n",
        "  Modified from https://github.com/Sairamvinay/Fake-News-Dataset\n",
        "  \"\"\"\n",
        "  MAX_FEATURES = 10000\n",
        "  # initalise vecoriser\n",
        "  vectoriser = CountVectorizer(max_features = MAX_FEATURES)\n",
        "  # fit on data\n",
        "  vectoriser.fit(training_text)\n",
        "\n",
        "  # a dictonary of the counts of the top 10000 words\n",
        "  vocab = vectoriser.vocabulary_\n",
        "\n",
        "  X_train = vectoriser.transform(training_text)\n",
        "\n",
        "  # return a matrix\n",
        "  X_train = X_train.todense()\n",
        "\n",
        "  return X_train"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0JatJ5kbuI4"
      },
      "source": [
        "def TFIDF(training_text):\n",
        "\n",
        "  \"\"\"\n",
        "  Uses TfidfVectorizer to vectorise text.\n",
        "  Modified from https://github.com/Sairamvinay/Fake-News-Dataset\n",
        "  \"\"\"\n",
        "  MAX_FEATURES = 10000\n",
        "\n",
        "  vectoriser = TfidfVectorizer(max_features = MAX_FEATURES)\n",
        "  \n",
        "  vectoriser.fit(training_text)\n",
        "  \n",
        "  X_train = vectoriser.transform(training_text)\n",
        "\n",
        "  X_train = X_train.todense()\n",
        "\n",
        "  return X_train\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}